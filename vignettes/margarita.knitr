%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Using margarita with texmex}
\documentclass[a4paper]{article}
\usepackage[british]{babel}
\begin{document}
\title{margarita: making texmex more digestible}
\author{Harry Southworth}
\date{}
\maketitle

\section{Introduction}
This document illustrates some usage of the margarita
package for R~\cite{R}. The margarita package is designed to simplify extreme
value modelling of clinical trial lab safety data, as described by Southworth
\& Heffernan~\cite{southHeff}. For non-clinical trial use, the package also
provides ggplot~\cite{ggplot2} replacement functions for several of the
texmex~\cite{texmex} plot functions.

Development of the margarita package was partially funded by AstraZeneca who have
kindly made it publicly available.

The major gains in efficiency in the analysis of clinical trial safety data come
from simpler simulation of return levels and upper limit of normal exceedance
probabilities, as well as functions to support a standard approach to
robust regression modelling.

Attempts are made at automatic threshold selection, including use of the
extended generalized Pareto 3 (EGP3)
distribution of Papastathopoulos \& Tawn~\cite{egp}. However, this functionality
relies on known context of clinical trial safety data and might not work well in
other contexts. In any event, automatic threshold selection appears to be a tricky
matter~\cite{scarrottMacDonald} and standard diagnostic plots should always be studied.

Further, margarita attempts to make the restructuring of CDISC STDM lab data
straightforward, though this functionality is based on AstraZeneca's implementation
of STDM and might be inappropriate or need adjustment for other implementations.

<<width, echo=FALSE>>=
options(width=60)
@

\section{Data importation}
Typically, the data will exist in SAS datasets with file extensions {\tt .sas7bdat}.
In what follows, we assume that the available SAS datasets are uncompressed and do
not rely on format catalogues. If in your particular case those circumstances do
not hold, you might want to explore some of the options discussed below. With
increasing amounts of outsourcing and inlicensing, it is likely that these such
alternatives will prove necessary more commonly in the future.

Supposing the SAS datasets to be held in directory {\tt user/data/} (the path
being relative to the R workspace in use), we import using the following commands.

<<data, echo=TRUE, eval=FALSE>>=
library(margarita)
library(sas7bdat)
dataPath <- "/user/data/"
dm <- read.sas7bdat(paste0(dataPath, "dm.sas7bdat"))
lb <- read.sas7bdat(paste0(dataPath, "lb.sas7bdat"))
@

We can examine the first few rows, and summarize the data as follows:

<<attach, echo=FALSE>>=
# Previous codeblock was not evaluated so need to attach here.
library(margarita)
library(sas7bdat)
@

<<summary, echo=TRUE>>=
head(dm)
summary(dm)
@

\subsection{Non-standard SAS datasets}
SAS dataset can be troublesome if SAS formats catalogues (particularly broken
ones) are used, or if compressed SAS data is used. SAS do not publish the
application programming interface for their datasets, nor do they publish their
compression algorithm. Nevertheless, attempts have been made to reverse engineer
the {\tt .sas7bdat} data files.

The sas7bdat~\cite{sas7bdat} package for R, by Matt Shotwell, works well at importing
many SAS datasets into R, but does not attempt to work with compressed datasets.
The SassyReader~\cite{sassyreader} project rewrote Shotwell's code into a Java
library but still did not attempt to cope with compressed formats. More recently,
GGA Software Services released their open source Parso~\cite{parso} Java library
which appears to do a good job of reading SAS datasets, compressed or otherwise,
and writing to comma separated values. In turn, Shotwell has now written an R
package, sas7bdat.parso~\cite{sas7bdat.parso} to act as a wrapper to the GGA
Parso library.

Finally, it might be the case that expected columns are missing from the data, such
as the number of days on study. Whilst, in general, it would be better to construct
such columns in accordance with existing standards using SAS, the margarita
package contains a few helper functions in order to reduce reliance on alternative
software and personnel.

\subsection{Data aggregation}
Typically, the lab dataset will contain data on a lot of lab variables. In our
current case, we are interested only in ALT, so we subset down on that.

<<subset, echo=TRUE>>=
alt <- lb[lb$lbtestcd == "ALT", ]
@

We now reduce the data to the baseline values and post-baseline maxima. See the
help files for {\tt getBaselines} and {\tt getAggregateData} for further information.

<<alt, echo=TRUE>>=
alt <- getBaselines(alt, visit="visitnum")
alt <- getAggregateData(alt)
@

Examining the help file for {\tt getAggregateData} gives more details of the
options allowed. By default, the function assumes the test name to be held in
a column called `lbtest' and the analysis values to be held in `aval'.
The function as called above assumes that the baseline
column does not exist. If that is not the case, use the {\tt baseline} argument to
override the function's attempt to reconstruct the baseline data.

If the {\tt lb} dataset already contains the treatment decodes and population
flags, the {\tt dm} dataset will not likely be required. Supposing, though,
that the treatment decodes need to be added to {\tt lb} we use

<<addvars, echo=TRUE>>=
alt <- addVariables(alt, dm, vars="armcd")
@

The above code assumes that the unique subject identifier is called `usubjid'
and that the treatment group identifier is called `armcd'. See the help file
for {\tt addVariables} for details on more control over the function:

<<help, echo=TRUE, eval=FALSE>>=
?addVariables
@

Note that it will often be the case that additional steps are necessary to tidy
up the analysis dataset, such as removing screen failures and ensuring that
`param' or `lbtestcd' really does uniquely identify lab tests (e.g. that they
don't need to be combined with a column indicating if the source was serum, urine,
or something else).

Finally, note that subjects who have no baseline value recorded, or no post-baseline
value recorded, cannot be used in the analysis (unless some form of multiple
imputation is to be employed, but in the extreme value setting it is not
obvious how this would be done). As such, the dataset returned by
{\tt getAggregateData} drops all such cases with missing values.

We should now have an analysis-ready dataset, the first few rows of which look
something like this:

<<head, echo=TRUE>>=
head(alt)
@

Shiftplots of the data appear in Figure~\ref{fig:shiftplots}.

<<shiftplots, echo=TRUE, fig.cap="Shiftplots of the liver data.">>=
shiftplot(alt, aes(baseline, aval), by=~armcd,
          xlab="Baseline ALT (U/L)",
          ylab="Maximum ALT (U/L)")
@

\clearpage
\section{Robust regression}
Robust regression can be performed using the {\tt lmr}
(linear model robust) function. This function is a
wrapper to {\tt rlm} in the MASS~\cite{MASS} package, setting the
method to MM, efficiency for Gaussian data to 85\%, and using bisquare weight
functions. It also computes
the coefficient covariance matrix and attaches it to the
returned object. A {\tt ggplot} function for diagnostics is
provided.

The use of MM-estimation with the particular settings as described above is
used as default as recommended by Maronna et al~\cite{maronnaMartinYohai} and
as described by Southworth \& Heffernan~\cite{southHeff}.

In our data, dose is ordered such that $A < B < C < D$. We take advantage of
this structure by assuming linearity in dose and recoding A -- D as 1 -- 4.

<<lmr, echo=TRUE, fig.cap="Diagnostic plots from the robust regression of the ALT data.">>=
alt$ndose <- as.numeric(alt$armcd)
mm <- lmr(log(aval) ~ log(baseline) + ndose, data=alt)
ggplot(mm)
@

The reason for using {\em robust} regression is that we do not expect the data to be
normally distributed, but to have outliers. Whilst most robust regression methods
implicitly assume the distribution to be symmetric, in practice parameter estimates
are very little affected by skew in the data. In Figure~\ref{fig:lmr}, it can be
seen in the QQ-plot (top left panel) that the residuals drift away from the reference
line to the bottom left, but more so to the top right. This indicates outliers
to the right, confirming the lack of normality in the data. The outliers can
also be seen in the other panels of Figure~\ref{fig:lmr}. The bottom left panel
shows plots of residuals versus fitted values and the smoother should be near-horizontal
(excusing end-effects) through the bulk of the data. The bottom right panel shows
the observed values versus the fitted values and should show (as it does) a near linear
relationship with little evidence of non-constant variance.

We can also look at boxplots of scaled residuals by treatment group.
<<boxplots, echo=TRUE, fig.cap="Boxplots of residuals from the robust regression, by dose group.">>=
boxplot(mm, by="armcd")
@
\clearpage
In Figure~\ref{fig:boxplots} we can see a fairly clear dose-response relationship
with there being few outliers in the lower dose groups, but more, and larger,
outliers in the higher dose groups. In practice, it is often at this stage of the
analysis that the presence or absence of treatment effects begins to become clear.
Note that the residuals in Figure~\ref{fig:boxplots} have been scaled so that values
beyond about $\pm 2.5$ are inconsistent with normality, justifying the use
of robust estimation.

\section{Extreme value modelling}
We now move on to fit generalized Pareto (GP) distributions to the
residuals from the robust regression.

\subsection{Threshold selection}
Before fitting GP models, we need to select a threshold above which GP models
are likely to be reasonable. We proceed firstly by producing some plots to
aid threshold selection.

<<thresh, echo=TRUE, fig.cap="Threshold selection plots for the residual ALT values.">>=
# Do threshold selection plots
alt$r <- resid(mm)
ggplot(gpdThresh(alt$r))
@

The top row of Figure~\ref{fig:thresh} displays plots of the scale and shape
parameters in the GP distribution above increasing thresholds.
Following Coles~\cite{coles}, these should be constant above a threshold suitable
for GP modelling and, to make use of as much data as possible, we want to choose
the lowest suitable threshold. Accounting for the uncertainty represented by the
approximate pointwise confidence intervals on the graphs, a threshold close to
zero, or perhaps a little higher, looks like it should be ok.

The lower left panel of Figure~\ref{fig:thresh} shows the {\em mean residual life}
(MRL) plot, again discussed by Coles~\cite{coles}. The MRL plot should be linear
above a suitable threshold, uncertainty accounted for. In practice, interpretation
of such plots requires some practice, but in this case a threshold close to zero
seems like it might be reasonable.

The bottom right panel of Figure~\ref{fig:thresh} contains a threshold selection
plot suggested by Papastathoploulos and Tawn~\cite{egp}. The horizontal axis
is a range of thresholds and the vertical axis is the value of estimated
power parameter $\kappa$ in the EGP3 distribution. At the point where
$\kappa = 1$, the EPG3 distribution is identical to the GP distribution, suggesting
that the lowest threshold for which the confidence interval for $\hat\kappa$
contains 1 as a candidate. It can be seen that the displayed pointwise confidence
interval crosses or comes close to the horizontal reference line at 1 at thresholds
around zero, 0.125 and then above about 0.3. Again, this suggests that a threshold
of zero or a little higher might be appropriate.

In Figure~\ref{fig:thresh}, the data are {\em all} of the residuals from the robust
regression with no account being taken of dose (or any other covariate). As such,
they can only be considered to be approximations of what we really want, which
is a threshold above which GP modelling accounting for dose is appropriate.

\subsubsection{A test for threshold selection}
Papastathopoulos and Tawn~\cite{egp} suggest selecting the lowest threshold above
which the confidence interval for $\hat\kappa$ contains 1. In the context of
clinical trials, we typically have fewer observations than in other settings, so
some care is needed in applying this approach.

Because we are modelling residuals from a robust linear model, the shape of the
distribution of those residuals should be not far from symmetric and should be
centered close to zero. This suggests that any threshold beneath the median of
the residuals would be inappropriate because the shape above a lower threshold
would not be compatible with the shape of the GP distribution.

Also, due to the sample sizes we expect in practice, if we reject all the
residuals beneath about the $75^{th}$ percentile, then we will be left with too
few to do any reasonable modelling with.

As such, when testing for a threshold, we restrict attention to values between
the median and the $75^{th}$ percentile of the residuals (though these default
values can be changed).

Further details of the implementation of the EGP3 test in margarita are as follows.
The function {\tt egp3Thresh} calls on {\tt egp3RangeFit} from the texmex
package. It then interpolates through the values of $\hat\kappa$ and its confidence
intervals before looking for the lowest threshold at which the confidence interval
contains 1, and returning that threshold as well as the threshold for which
$\hat\kappa$ is closest to 1. If no confidence interval contains 1, the function
issues a warning.

We now try the test on the residual ALT data. As with the threshold selection
plots in Figure~\ref{fig:thresh}, the test can be considered only approximate
because it takes no account of dose, which is what we are actually interested in.

<<test, echo=TRUE>>=
egp3Thresh(alt$r)
@

None of the confidence intervals contained 1, so we got a warning and a value
of {\tt Inf}. The threshold for which the value of $\hat\kappa$ is closest to
1 is 0.0967. This value corresponds with what we can read off the lower right
panel of Figure~\ref{fig:thresh}. Approximately
\Sexpr{round(mean(alt$r < .0967)*100)}\% of the residuals are beneath this value,
suggesting a slightly lower threshold than the $70^{th}$ percentile used by
Southworth \& Heffernan~\cite{southHeff}.

\subsection{Fitting generalized Pareto distributions}
Following the discussion in the previous section, we proceed to
fit GP models to the residual ALT values by maximum likelihood and look
at diagnostic plots. We threshold at the $60^{th}$ percentile.

<<gpd, echo=TRUE, fig.cap="Diagnostic plots for the GPD model fit to the residual ALT values without accounting for dose.">>=
mlmod <- evm(r, data=alt, qu=.6)
ggplot(mlmod)
@

Diagnostic plots appear in Figure~\ref{fig:gpd}. No account has been taken of
dose, so these plots can only be considered as approximate guidance on whether
GP models are likely to fit well above the chosen threshold. The pointwise
simulated envelopes give no cause for concern. In general, the QQ-plot (the
top right panel) is usually the one in which any lack of model fit becomes most
clear.

Following Southworth \& Heffernan~\cite{southHeff}, we wish to account for dose.
As with the robust regression modelling previously, we assume linearity in dose.
Either one, both or none of the parameters in the GP might be dependent on dose,
so we fit a variety of models and compare them using the Akaike Information
Criterion (AIC). The AIC provides guidance on model selection, but not a hard
rule, so in some cases, we might want to consider the change in deviance due to
the addition of a single parameter more carefully. Table~\ref{tab:deviance}
is reproduced from Kass \& Raftery~\cite{kassRaftery}, though versions of it have
been suggested by many other authors~\cite{jeffreys, raftery, lindsey, davison, royall}.

<<deviance, echo=FALSE, results="asis">>=
evidence()
@

<<gpd2, echo=TRUE, results="asis">>=
gpd0 <- evm(r, alt, qu=.6) # null model
gpd1 <- evm(r, alt, qu=.6, phi=~ndose)
gpd2 <- evm(r, alt, qu=.6, xi=~ndose)
gpd3 <- evm(r, alt, qu=.6, phi=~ndose, xi=~ndose)
AICtable(list(gpd0, gpd1, gpd2, gpd3))
@

The function {\tt AICtable} provides a neat way of producing \LaTeX output
containing the AICs, log-likelihoods and related information in a written report
(like this one). Alternatively, examine any of the fitted models by printing
them:

<<gmod2, echo=TRUE>>=
gpd2
@

From Table~\ref{tab:aic}, the model with a linear term for dose in $\xi$ is
preferred by AIC. We check the diagnostic plots for the MLE fit before refitting
using Markov chain Monte Carlo (MCMC).

<<diag, echo=TRUE, fig.cap="Diagnostic plots for the chosen model.">>=
ggplot(gpd2)
@
\clearpage
The diagnostic plots in Figure~\ref{fig:diag} give no cause for concern.

In this example, there is a linear dose-response relationship, with
$\hat\xi$ increasing as dose increases. Such a relationship makes logical sense.
In some examples, it might be the case that a decreasing or non-monotone
dose-response is suggested. Such a relationship is unlikely to make sense
and the analyst then needs to weight the strength of evidence in the data
(i.e considerations of AIC and deviance) with their knowledge of the data
generating mechanism (i.e. the treatment regimens). Statistical inference
is only one part of the larger process of scientific data modelling.

<<bmod, echo=TRUE>>=
gmod <- evm(r, alt, qu=.6, xi=~ndose, method="sim")
@

Note that fitting using MCMC results (by default) in the function telling the 
user the acceptance rate. For fairly simple models with few parameters, as we
have here, an acceptance rate of a little over 30\% is common. If the rate is
much higher or lower than this, then something might have gone wrong. Also, if
more parameters are included in the model, the acceptance rate should be
expected to go down. Section 12.2 of Gelman et al~\cite{bda} contains some
discussion of acceptance rates and efficient MCMC algorithms.

Having refit, we now look at diagnostic plots for the Markov chains:

<<markov, echo=TRUE, fig.cap="Diagnostic plots for the Markov chains.">>=
ggplot(gmod)
@

The top row of Figure~\ref{fig:markov} shows kernel density estimates of the
posterior distributions as simulated by the Markov chains. The second row shows
the simulated values at each step of the chain and the cumulative means. If the
Markov chains have converged on their target distributions, the cumulative means
should converge rapidly on stable values and the trace of the chains should look
like `fat hairy caterpillars'. The final row of Figure~\ref{fig:markov} shows
the autocorrelation functions (ACFs) of the chains. If the algorithm has worked
well, the ACFs should rapidly decay to zero.

In general, it is good practice to discard the first part of the Markov chains
(the {\em burn-in} period) and to {\em thin} the chains by discarding values at
regular intervals. For example, we could keep in in every 4 values from the
chains. These steps improve the chances that the chains have converged on the
target distribution, reduce autocorrelation, and make the simulated values
behave more like a random sample from the target distribution. The help file
for {\tt evm} describes the {\tt thin} and {\tt burn} arguments that control
these settings.

\clearpage
\section{\tt margarita}
The main efficiency that margarita package introduces to the analysis of clinical
trial safety data is to provide built-in functions to aid simulation of
quantities of interest such as return levels and probabilities of exceeding
high values. This is achieved by first creating an object that contains the
robust linear model, the MCMC simulated GP model, and other information
required for the calculations.

<<margarita, echo=TRUE>>=
nd <- data.frame(ndose=1:4)
mods <- margarita(mm, gmod, newdata=nd,
                  trans=log, invtrans=exp,
                  baseline="baseline")
@

In the call to {\tt margarita}, we passed in the robust regression model,
the MCMC extreme value model. The resulting margarita object is then interpreted
by {\tt simulate} functions and used to generate predicted values of interest.

The {\tt simulate} functions infer from the number of draws from the
posterior distributions contained in the extreme value model how many cases to
simulate.

If there are no covariates in the model, {\tt simulate}
doesn't need the {\tt newdata} argument. However, if there
are covariates {\em in either the robust linear model, the GP
model or both}, in order to deal with all kinds of trial designs, the
{\tt simulate} function requires {\tt newdata}. The rows of {\tt newdata} should
be unique and it should {\emph not} contain the baseline data. Baseline data are
simulated internally by resampling from the data attached to the linear
model object. The rows of {\tt newdata} therefore represent combinations of
covariates in which the user is interestd in.

The {\tt trans} argument tells the simulation function how the response variable
is transformed in the robust linear model, and the {\tt invtrans} argument gives
the reverse transformation. These default to {\tt log} and {\tt exp}, but in some
cases might both be {\tt I} for the identity (i.e. no) transformation, or to
{\tt sqrt} and {\tt function(x) x*x} for the square root and square transforms.
Note that {\tt trans} and {\tt invtrans} are functions, not character strings.

\section{Return levels}
Return levels for clinical trial data can be simulated
using the {\tt simulate} function with {\tt type='rl'}
(the default). The simulations for various return levels
can be run in a single call by passing a vector argument
into the function.

Note that it is common for clinical laboratory safety data to be interpreted
in terms of how far the observed values are from being `normal'. As such, each
lab variable tends to be issued with an upper limit of normal (ULN) and a lower
limit of normal (LLN), though we are usually concerned only with one or the other.

There is a great deal of arbitrariness in the computation of the LLN and ULN.
The values ought to represent a low and a high quantile of the distribution in
a healthy population, but they vary from lab to lab, partly due to the machines
that make the readings differing from lab to lab, partly because the local
populations differ from lab to lab, and because the way that low and high
quantiles are computed differs. M'Kada et al~\cite{mkada} describe one lab
that calculated the ULN for ALT as the mean plus two standard deviations after
removing the 5\% most extreme values, but later recalculated it as the mean
plus a single standard deviation after removing the 5\% most extreme values.
Given that ALT is skewed and subject to outliers, it is difficult to justify
either approach.

To avoid the risk of increasing variation by using local lab normal ranges, it
is recommended to use fixed ranges as published in the public domain. A little
searching reveals estimated ULNs for ALT ranging from 26 U/L to 72 U/L. The
Merck manual~\cite{merck} suggests 35 U/L, and it is recommended that the Merck
manual be the first place to look when attempting to decide on a reasonable
reference range.

Finally, in clinical trials we observe patients at baseline (i.e. before
treatment) and then after a period of treatment. Baseline values are almost
always correlated with on-treatment values, so a more appropriate approach
to interpreting the effect of drug on lab values would be to eliminate the
baseline effect and study what remains. An approach that comes close to this
is to look at changes from baseline. In practice, we work with the residuals
from a robustly fit linear model, but transform results back to the scale
of the change from baseline and multiples of ULN.

<<rl, echo=TRUE>>=
ULN <- 35

rl1000 <- simulate(mods, type="rl", M=1000)

s <- summary(rl1000)
suln <- s / ULN
sFold <- summary(rl1000, scale="proportional")
sDiff <- summary(rl1000, scale="difference")

# s and the others are lists with an element for each element of M
rownames(s[[1]]) <- paste("Dose", LETTERS[1:4])
rownames(sFold[[1]]) <- rownames(sDiff[[1]]) <-
  rownames(suln[[1]]) <- rownames(s[[1]])
gs <- ggplot(s, xlab="U/L")
gsuln <- ggplot(suln, xlab="Multiples of ULN")
gsFold <- ggplot(sFold, xlab="Fold-change from baseline")
gsDiff <- ggplot(sDiff, xlab="Change in U/L from baseline")
grid.arrange(gs, gsuln, gsFold, gsDiff)
@

Notice that for return levels, because of the way they are
calculated, it is straightforward to change the scale
after the calculations have been done. It is achieved by
providing optional arguments to {\tt summary} or by
manipulating the object returned by {\tt summary} directly.

Here we have included the absolute change from baseline in the output, though
for ALT it is usually the proportional change that is of more interest.

\section{Threshold exceedance probabilities}
When simulating threshold exceedance probabilities, the
scale of the predicted values needs to be specified in the
call to {\tt simulate} rather than {\tt summary}.

First we find probabilities of threshold exceedance on the
scale of the raw data (i.e. multiples of ULN), then in terms
of multiples of baseline.

<<prob, echo=TRUE>>=
pULNs <- simulate(mods, type="prob",
                  M=ULN * c(1, 3, 10, 20),
                  Mlabels=c("P(ALT > ULN)", "P(ALT > 3xULN)",
                            "P(ALT > 10xULN)", "P(ALT > 20xULN)"))
ps <- summary(pULNs)

# Manually set the treatment group names
names(ps) <- LETTERS[1:4]
ggplot(ps, ncol=1)

# Simulate fold-changes from baseline
pBase <- simulate(mods, type="prob", M=c(2, 5, 10, 20),
                  Mlabels=c("2-fold", "5-fold",
                            "10-fold", "20-fold"),
                  scale="proportional")
pbs <- summary(pBase)

# Manually change treatment group names
names(pbs) <- LETTERS[1:4]
ggplot(pbs, ncol=1,
       xlab="Proportion of patients with exceedance")
@

\bibliographystyle{plain}
\bibliography{texmex}
\end{document}
